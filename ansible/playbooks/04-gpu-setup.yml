# Setup NVIDIA GPU support on GPU nodes using GPU Operator
# Reference: https://docs.rke2.io/add-ons/gpu_operators
---
- name: Install NVIDIA drivers and container toolkit on GPU nodes
  hosts: gpu_nodes
  become: yes

  tasks:
    - name: Add NVIDIA package signing key
      apt_key:
        url: https://nvidia.github.io/libnvidia-container/gpgkey
        state: present

    - name: Add NVIDIA container toolkit repository
      apt_repository:
        repo: "deb https://nvidia.github.io/libnvidia-container/stable/deb/$(ARCH) /"
        state: present
        filename: nvidia-container-toolkit

    - name: Install NVIDIA container toolkit
      apt:
        name:
          - nvidia-container-toolkit
        state: present
        update_cache: yes

    - name: Configure containerd for NVIDIA runtime
      shell: |
        nvidia-ctk runtime configure --runtime=containerd --config=/var/lib/rancher/rke2/agent/etc/containerd/config.toml
      notify: Restart RKE2 agent

    - name: Verify NVIDIA driver is loaded
      command: nvidia-smi
      register: nvidia_smi
      changed_when: false

    - name: Display GPU info
      debug:
        var: nvidia_smi.stdout_lines

  handlers:
    - name: Restart RKE2 agent
      systemd:
        name: rke2-agent
        state: restarted

- name: Deploy NVIDIA GPU Operator to cluster
  hosts: control_plane
  become: yes

  tasks:
    - name: Create GPU Operator HelmChart manifest
      copy:
        dest: /var/lib/rancher/rke2/server/manifests/gpu-operator.yaml
        content: |
          apiVersion: helm.cattle.io/v1
          kind: HelmChart
          metadata:
            name: gpu-operator
            namespace: kube-system
          spec:
            repo: https://helm.ngc.nvidia.com/nvidia
            chart: gpu-operator
            version: v25.3.0
            targetNamespace: gpu-operator
            createNamespace: true
            valuesContent: |-
              toolkit:
                env:
                - name: CONTAINERD_CONFIG
                  value: /var/lib/rancher/rke2/agent/etc/containerd/config.toml
                - name: CONTAINERD_SOCKET
                  value: /run/k3s/containerd/containerd.sock
                - name: CONTAINERD_RUNTIME_CLASS
                  value: nvidia
                - name: CONTAINERD_SET_AS_DEFAULT
                  value: "true"
              driver:
                enabled: false  # Using pre-installed drivers
              devicePlugin:
                config:
                  name: time-slicing-config
                  default: any
        mode: '0644'

    - name: Wait for GPU Operator namespace to be created
      shell: |
        export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        kubectl get namespace gpu-operator
      register: gpu_operator_ns
      retries: 30
      delay: 10
      until: gpu_operator_ns.rc == 0
      ignore_errors: yes

    - name: Wait for GPU Operator pods to start
      shell: |
        export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        kubectl -n gpu-operator get pods
      register: gpu_operator_pods
      retries: 30
      delay: 10
      until: gpu_operator_pods.rc == 0
      ignore_errors: yes

    - name: Display GPU Operator pods
      debug:
        var: gpu_operator_pods.stdout_lines
      when: gpu_operator_pods is defined

    - name: Wait for GPU resources to be advertised (may take several minutes)
      shell: |
        export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        kubectl get nodes -o json | jq -r '.items[] | select(.status.capacity["nvidia.com/gpu"] != null) | .metadata.name'
      register: gpu_nodes_ready
      retries: 30
      delay: 20
      until: gpu_nodes_ready.stdout != ""
      ignore_errors: yes

    - name: Verify GPU resources are advertised
      shell: |
        export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
        kubectl get nodes -o custom-columns="NAME:.metadata.name,GPU:.status.capacity.nvidia\.com/gpu"
      register: gpu_resources

    - name: Display GPU resources per node
      debug:
        var: gpu_resources.stdout_lines
