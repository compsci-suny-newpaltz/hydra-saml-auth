# vLLM Cluster - Chimera Head Node
# IP: 192.168.1.150
# GPUs: 3x RTX 3090 (72GB VRAM)

services:
  ray-head:
    image: rayproject/ray:2.35.0-py311-gpu
    container_name: ray-head
    hostname: chimera-ray
    network_mode: host
    ipc: host
    privileged: true
    volumes:
      - /dev/infiniband:/dev/infiniband:ro
    environment:
      - RAY_HEAD_NODE=1
    command: ray start --head --port=6379 --dashboard-host=0.0.0.0 --dashboard-port=8265 --block
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    network_mode: host
    ipc: host
    privileged: true
    volumes:
      - /models:/root/.cache/huggingface
      - /dev/infiniband:/dev/infiniband:ro
    environment:
      # MXFP4 quantization for MoE models
      - VLLM_USE_FLASHINFER_MXFP4_MOE=1
      # Soft-RoCE configuration (for software RDMA)
      # Uncomment if using Soft-RoCE:
      # - NCCL_IB_HCA=rxe0
      # - NCCL_NET_GDR_LEVEL=0
      # - NCCL_IB_GID_INDEX=3
      # Hardware RDMA configuration (for Mellanox):
      # - NCCL_IB_HCA=mlx5_0
      # Standard TCP fallback (current setup):
      - NCCL_SOCKET_IFNAME=enp71s0
      - NCCL_DEBUG=INFO
      - CUDA_VISIBLE_DEVICES=0,1,2
      - VLLM_ATTENTION_BACKEND=FLASH_ATTN
    # Example: Run a smaller model first to test
    command: >
      vllm serve "meta-llama/Llama-3.1-8B-Instruct"
      --tensor-parallel-size 3
      --port 8000
      --host 0.0.0.0
      --max-model-len 4096
    # For gpt-oss-120b (requires multi-node):
    # command: >
    #   vllm serve "openai/gpt-oss-120b"
    #   --tensor-parallel-size 3
    #   --pipeline-parallel-size 2
    #   --trust-remote-code
    #   --max-model-len 4096
    #   --port 8000
    #   --host 0.0.0.0
    depends_on:
      - ray-head
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

volumes: {}
